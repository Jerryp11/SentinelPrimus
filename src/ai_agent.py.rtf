{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww50700\viewh25200\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 # src/ai_agent.py\
\
from flask import Flask, request, jsonify\
import openai\
import re\
import os\
\
app = Flask(__name__)\
\
# Load OpenAI API Key from environment\
openai.api_key = os.getenv("OPENAI_API_KEY")\
\
# Simple guardrail to detect basic prompt injection\
def is_malicious_input(user_input):\
    suspicious_patterns = [\
        r"ignore previous", r"you are now", r"disregard instructions",\
        r"pretend to be", r"system prompt", r"admin override"\
    ]\
    for pattern in suspicious_patterns:\
        if re.search(pattern, user_input, re.IGNORECASE):\
            return True\
    return False\
\
@app.route("/ask", methods=["POST"])\
def ask_agent():\
    data = request.get_json()\
    user_input = data.get("message", "")\
\
    if is_malicious_input(user_input):\
        return jsonify(\{\
            "status": "blocked",\
            "message": "\uc0\u9888 \u65039  Request blocked due to policy violation."\
        \})\
\
    try:\
        response = openai.ChatCompletion.create(\
            model="gpt-4",\
            messages=[\{"role": "user", "content": user_input\}]\
        )\
        reply = response.choices[0].message['content']\
        return jsonify(\{"status": "success", "response": reply\})\
    except Exception as e:\
        return jsonify(\{"status": "error", "message": str(e)\})\
\
if __name__ == "__main__":\
    app.run(debug=True)\
}